{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Splinter (Supabase Postgres LINTER)","text":"<p>This project maintains a set of lints for Supabase projects. It uses SQL queries to identify common database schema issues. Some lints are general purpose for Postgres projects while others are specific to Supabase features, storing their data in Postgres e.g. auth and storage.</p>"},{"location":"0001_unindexed_foreign_keys/","title":"Unindexed Foreign Keys","text":"<p>Level: INFO</p>"},{"location":"0001_unindexed_foreign_keys/#rationale","title":"Rationale","text":"<p>In relational databases, indexing foreign key columns is a standard practice for improving query performance. Indexing these columns is recommended in most cases because it improves query join performance along a declared relationship.</p>"},{"location":"0001_unindexed_foreign_keys/#what-is-a-foreign-key","title":"What is a Foreign Key?","text":"<p>A foreign key is a constraint on a column (or set of columns) that enforces a relationship between two tables. For example, a foreign key from <code>book.author_id</code> to <code>author.id</code> enforces that every value in <code>book.author_id</code> exists in <code>author.id</code>. Once the foriegn key is declared, it is not possible to insert a value into <code>book.author_id</code> that does not exist in <code>author.id</code>. Similarly, Postgres will not allow us to delete a value from <code>author.id</code> that is referenced by <code>book.author_id</code>. This concept is known as referential integrity.</p>"},{"location":"0001_unindexed_foreign_keys/#why-index-foreign-key-columns","title":"Why Index Foreign Key Columns?","text":"<p>Given that foreign keys define relationships among tables, it is common to use foreign key columns in join conditions when querying the database. Adding an index to the columns making up the foreign key improves the performance of those joins and reduces database resource consumption.</p> <pre><code>select\nbook.id,\nbook.title,\nauthor.name\nfrom\nbook\njoin author\n-- Both sides of the following condition should be indexed\n-- for best performance\non book.author_id = author.id\n</code></pre>"},{"location":"0001_unindexed_foreign_keys/#how-to-resolve","title":"How to Resolve","text":"<p>Given a table:</p> <pre><code>create table book (\nid serial primary key,\ntitle text not null,\nauthor_id int references author(id) -- this defines the foreign key\n);\n</code></pre> <p>To apply the best practice of indexing foreign keys, an index is needed on the <code>book.author_id</code> column. We can create that index using:</p> <pre><code>create index ix_book_author_id on book(author_id);\n</code></pre> <p>In this case we used the default B-tree index type. Be sure to choose an index type that is appropriate for the data types and use case when working with your own tables.</p>"},{"location":"0001_unindexed_foreign_keys/#example","title":"Example","text":"<p>Let's look at a practical example involving two tables: <code>order_item</code> and <code>customer</code>, where <code>order_item</code> references <code>customer</code>.</p> <p>Given the schema:</p> <pre><code>create table customer (\nid serial primary key,\nname text not null\n);\n\ncreate table order_item (\nid serial primary key,\norder_date date not null,\ncustomer_id integer not null references customer (id)\n);\n</code></pre> <p>We expect the tables to be joined on the condition</p> <pre><code>customer.id = order_item.customer_id\n</code></pre> <p>As in:</p> <pre><code>select\ncustomer.name,\norder_item.order_date\nfrom\ncustomer\njoin order_item\non customer.id = order_item.customer_id\n</code></pre> <p>Using Postgres' \"explain plan\" functionality, we can see how its query planner expects to execute the query.</p> <pre><code>Hash Join  (cost=38.58..74.35 rows=2040 width=36)\n  Hash Cond: (order_item.customer_id = customer.id)\n  -&gt;  Seq Scan on order_item  (cost=0.00..30.40 rows=2040 width=8)\n  -&gt;  Hash  (cost=22.70..22.70 rows=1270 width=36)\n        -&gt;  Seq Scan on customer  (cost=0.00..22.70 rows=1270 width=36)\n</code></pre> <p>Notice that the condition <code>order_item.customer_id = customer.id</code> is being serviced by a <code>Seq Scan</code>, a sequential scan across the <code>order_items</code> table. That means Postgres intends to sequentially iterate over each row in the table to identify the value of <code>customer_id</code>.</p> <p>Next, if we index <code>order_item.customer_id</code> and recompute the query plan:</p> <pre><code>create index ix_order_item_customer_id on order_item(customer_id);\n\nexplain\nselect\ncustomer.name,\norder_item.order_date\nfrom\ncustomer\njoin order_item\non customer.id = order_item.customer_id\n</code></pre> <p>We get the query plan:</p> <pre><code>Hash Join  (cost=38.58..74.35 rows=2040 width=36)\n  Hash Cond: (order_item.customer_id = customer.id)\n  -&gt;  Seq Scan on order_item  (cost=0.00..30.40 rows=2040 width=8)\n  -&gt;  Hash  (cost=22.70..22.70 rows=1270 width=36)\n        -&gt;  Seq Scan on customer  (cost=0.00..22.70 rows=1270 width=36)\n</code></pre> <p>Note that nothing changed.</p> <p>We get an identical result because Postgres' query planner is clever enough to know that a <code>Seq Scan</code> over an empty table is extremely fast, so theres no reason for it to reach out to an index. As more rows are inserted into the <code>order_item</code> table the tradeoff between sequentially scanning and retriving the index steadily tip in favor of the index. Rather than manually finding this inflection point, we can hint to the query planner that we'd like to use indexes by disabling sequentials scans except where they are the only available option. To provides that hint we can use:</p> <pre><code>set local enable_seqscan = off;\n</code></pre> <p>With that change:</p> <pre><code>set local enable_seqscan = off;\n\nexplain\nselect\ncustomer.name,\norder_item.order_date\nfrom\ncustomer\njoin order_item\non customer.id = order_item.customer_id\n</code></pre> <p>We get the query plan:</p> <pre><code>Hash Join  (cost=79.23..159.21 rows=2040 width=36)\n  Hash Cond: (order_item.customer_id = customer.id)\n  -&gt;  Index Scan using ix_order_item_customer_id on order_item  (cost=0.15..74.75 rows=2040 width=8)\n  -&gt;  Hash  (cost=63.20..63.20 rows=1270 width=36)\n        -&gt;  Index Scan using customer_pkey on customer  (cost=0.15..63.20 rows=1270 width=36)\n</code></pre> <p>The new plan services the <code>order_item.customer_id = customer.id</code> join condition using an <code>Index Scan</code> on <code>ix_order_item_customer_id</code> which is far more efficient at scale.</p>"},{"location":"0002_auth_users_exposed/","title":"Auth Users Exposed","text":"<p>TODO. Pending feedback from auth team.</p>"},{"location":"0003_auth_rls_initplan/","title":"Auth RLS InitPlan","text":"<p>Level: WARNING</p>"},{"location":"0003_auth_rls_initplan/#rationale","title":"Rationale","text":"<p>Row-Level Security (RLS) policies are the mechanism for controlling access to data based on user roles or attributes. These policies frequently use the provided helper functions in the <code>auth</code> schema including <code>auth.uid()</code>, <code>auth.role()</code>, <code>auth.email()</code>, and <code>auth.jwt()</code> to retrieve information about the current querying user. Improperly written RLS policies can cause these functions to execute once-per-row, rather than once-per-query. While the <code>auth.&lt;value&gt;()</code> functions are efficient, if executed once-per-row they can lead to significant performance bottlenecks at scale.</p>"},{"location":"0003_auth_rls_initplan/#the-performance-issue","title":"The Performance Issue","text":"<p>When an RLS policy is applied to a query, the conditions specified in the policy are evaluated for each row that the query touches. This means that if a policy condition calls a helper function like <code>auth.uid()</code>, this function is executed repeatedly for every row. In queries affecting thousands of rows, this behavior can drastically reduce query performance, as the overhead of executing these functions adds up quickly.</p>"},{"location":"0003_auth_rls_initplan/#how-to-resolve","title":"How to Resolve","text":"<p>To optimize the performance of RLS policies using <code>auth</code> helper functions we aim to reduce the number of times the helper functions are called. This can be achieved by caching the result of the function call for the duration of the query. Instead of calling the function directly in the policy condition, you can wrap the function call in a subquery. This approach executes the function once, caches the result, and compares this cached value against the column values for all subsequent rows.</p> <p>For example, consider the policy:</p> <pre><code>create policy \"inefficient_document_access\" on documents\nto authenticated\nusing ( auth.uid() = creator_id );\n</code></pre> <p>In this policy, <code>auth.uid()</code> is called for every row in the <code>documents</code> table to check if the <code>creator_id</code> matches the current user's ID. If the number of rows in <code>documents</code> is 150,000 the <code>auth.uid()</code> function will be executed 150,000, potentially incurring over 3 seconds of overhead per query.</p> <p>If we wrap the <code>auth.uid()</code> call in a subquery:</p> <pre><code>create policy \"efficient_document_access\" on user_data\nto authenticated\nusing ( (select auth.uid()) = user_id );\n</code></pre> <p>Then auth.uid() is called only once at the beginning of the query execution, and its result is reused for each row comparison. That change reduces the overhead from a few seconds to a few microseconds with no impact on the result set.</p> <p>Since the output values for the <code>auth</code> helper functions are set on a per-query basis there is no downside to aggresively applying this performacne optimization.</p>"},{"location":"0004_no_primary_key/","title":"No Primary Key","text":"<p>Level: INFO</p>"},{"location":"0004_no_primary_key/#rationale","title":"Rationale","text":"<p>Tables in a relational database should ideally have a key that uniquely identifies a row within that table. Tables lacking of a primary key is often considered poor design, as it can lead to data anomalies, complicate data relationships, and degrade query performance.</p>"},{"location":"0004_no_primary_key/#what-is-a-primary-key","title":"What is a Primary Key?","text":"<p>A primary key is a single column or a set of columns that uniquely identifies each row in a table. </p> <p>Primary keys are important because they enable:</p> <ol> <li>Uniqueness and Integrity: Ensures that each row in the table is unique and identifiable.</li> <li>Performance: The database automatically creates an index for the primary key, improving query performance when retrieving or manipulating data based on the primary key.</li> <li>Relationships: Unique keys, like primary keys, are a prerequisite for defining foreign keys in other tables, which are critical for relational database design and efficient joins.</li> </ol>"},{"location":"0004_no_primary_key/#how-to-resolve","title":"How to Resolve","text":"<p>For a table that lacks a primary key, the resolution involves identifying a column (or a set of columns) that can uniquely identify each row and altering the table to designate those columns as the primary key.</p> <p>Given a table:</p> <pre><code>create table customer (\nid integer not null,\nname text not null,\nemail text not null\n-- Notice the lack of a PRIMARY KEY constraint\n);\n</code></pre> <p>If we assume <code>id</code> is unique for each customer, we can add a primary key constraint to the table using:</p> <pre><code>alter table customer add primary key (id);\n</code></pre> <p>If no single column can serve as a unique identifier, consider using a composite key. A composite key combines multiple columns to form a unique identifier for each row.</p> <p>Example:</p> <p>Consider a table event_log that logs user activities without a primary key:</p> <pre><code>create table event_log (\nuser_id integer not null,\nevent_time timestamp not null,\naction text not null\n-- A combination of user_id and event_time can uniquely identify rows\n);\n</code></pre> <p>To resolve the lack of a primary key and ensure that each log entry is uniquely identifiable, we can add a composite primary key on user_id and event_time:</p> <pre><code>alter table event_log add primary key (user_id, event_time);\n</code></pre> <p>Ensure every table has a primary key, even if it's a synthetic key that doesn't have a natural counterpart in the data model. When possible, use a simple fixed size types like <code>int</code>, <code>bigint</code>, and <code>uuid</code> as the primary key for maximum efficiency.</p>"},{"location":"0005_unused_index/","title":"Unused Index","text":"<p>Level: INFO </p>"},{"location":"0005_unused_index/#rationale","title":"Rationale","text":"<p>Unused indexes in a database are a silent performance issue. While indexes are important for speeding up search queries, every index also adds overhead to the database. This overhead occurs because the database must update each index whenever data in the indexed table are inserted, updated, or deleted. If an index is never used by your queries, it burdens the database with unnecessary work, which can slow down write operations and consume additional storage space.</p>"},{"location":"0005_unused_index/#what-is-an-index","title":"What is an Index?","text":"<p>An index in a database is similar to an index in a book. It allows the database to find data without scanning the entire table. An index is created on a column or a set of columns in a table. Queries that search or sort data based on these columns can find data more quickly and efficiently by referring the index instead of each row in the table.</p>"},{"location":"0005_unused_index/#what-are-unused-indexes","title":"What are Unused Indexes","text":"<p>Unused indexes are indexes that have not been accessed by any query execution plans. This might occur if indexes were created proactively to support potential future query patterns or if application usage patterns change after a schema migration.</p>"},{"location":"0005_unused_index/#how-to-resolve","title":"How to Resolve","text":"<p>Before deleting an index, it's important to confirm that the index is genuinely unused and was unintentionally created:</p> <ul> <li>Consider future usage patterns. An index might be unused now but could be critical for upcoming features or during specific times of the year.</li> <li>Test the impact of removing the index in a development or staging environment to ensure that performance or query plans are not adversely affected.</li> </ul> <p>To remove an unused index, use the <code>drop index</code> statement:</p> <pre><code>drop index &lt;schema_name&gt;.&lt;index_name&gt;;\n</code></pre> <p>Replacing <code>schema_name</code> and <code>index_name</code> with the actual names from your database.</p>"},{"location":"0006_multiple_permissive_policies/","title":"Multiple Permissive Policies","text":"<p>Level: WARNING</p>"},{"location":"0006_multiple_permissive_policies/#rationale","title":"Rationale","text":"<p>In Postgres, Row Level Security (RLS) policies control access to rows in a table based on the executing user. When multiple permissive policies are applied to the same tablequeries against the table, the user may have access to a selected row through any of the policies. This means that, in the worst case, all of the relevant RLS policies must be applied/tested before Postgres can determine if a row should be visible. At scale, these checks add significant overhead to SQL queries and can be a performance bottleneck.</p>"},{"location":"0006_multiple_permissive_policies/#row-level-security-policies","title":"Row Level Security Policies","text":"<p>RLS policies in Postgres are rules applied to tables that determine whether rows can be selected, inserted, updated, or deleted. These policies can be set to <code>PERMISSIVE</code> or <code>RESTRICTIVE</code>. Permissive policies allow actions unless explicitly restricted by a restrictive policy. When multiple permissive policies are defined for a table, they act in a cumulative manner \u2014 if any policy allows access, the access is granted. In other words, the policies compose with <code>OR</code> semantics.</p>"},{"location":"0006_multiple_permissive_policies/#risks-with-multiple-permissive-policies","title":"Risks with Multiple Permissive Policies","text":""},{"location":"0006_multiple_permissive_policies/#access-control","title":"Access Control","text":"<p>Multiple permissive policies on a table can make it challenging to accurately predict and control which rows are accessible to different users. This complexity can inadvertently lead to overly permissive access configurations, undermining data security and integrity.</p>"},{"location":"0006_multiple_permissive_policies/#performance","title":"Performance","text":"<p>Since any one of N permissive policies can provide a user access to a given table's row, in the worst case Postgres must execute all N policies to determine if a row should be visible. These multiple checks raise the probability of a query falling off an index and broadly increase the resource consumption of every query on the impacted table.</p>"},{"location":"0006_multiple_permissive_policies/#how-to-resolve","title":"How to Resolve","text":"<p>Consider a table <code>employee_data</code> with two permissive policies:</p> <p>Policy A allows access to employees in the same department. Policy B allows access to employees at or above a certain grade level.</p> <p>Our intention is for users to be able to see employee data for employees within their own department who are below the querying user's grade level.</p> <pre><code>-- Policy A\ncreate policy department_access on employee_data\nfor select\nusing (department = current_user_department());\n\n-- Policy B\ncreate policy grade_level_access on employee_data\nfor select\nusing (grade_level &lt;= current_user_grade_level());\n</code></pre> <p>The implementation contains a logic error. As written, every employee can see <code>employee_data</code> for every other employee within their departemnt. Similarly, every employee can see every other employee's data at or below their own grade level.</p> <p>To address this issue, we can combine the two policies.</p> <pre><code>drop policy department_access on employee_data;\ndrop policy grade_level_access on employee_data;\n\ncreate policy consolidated_access on employee_data\nfor select\nusing (\ndepartment = current_user_department()\nor grade_level &gt;= current_user_grade_level()\n);\n</code></pre> <p>In addition to addressing the logic bug, we have also improved the Postgres query planner's ability to inline the policy to check access to rows, which reduces the chance of the query falling off index.</p> <p>While consolidating RLS policies for a given role/action combination is a best practices, it is not a hard rule. If consolidating policies leads to unreadable SQL then you may opt to have multiple policies for maintainability.</p>"}]}